\section{Introduction}
\label{section:introduction}

The complexity of today's technical systems is continuously increasing. Future systems will consist of a multitude of autonomous soft- and hardware components that interact with each other to satisfy functional requirements of the global system. Since this trend bears the risk of unpredictable or even uncontrollable system behavior, \emph{organic computing} \cite{RMB+06} % \cite{Sch05b} % \footnote{http://www.organic-computing.de/spp} % {We gratefully acknowledge the financial support by the German Research Foundation (DFG) within the priority programme 1183 Organic Computing.} 
focuses on monitoring, analyzing, and controlling complex distributed systems to endow them with the ability of \emph{controlled self-organization}. An or\-ga\-nic system can self-organize to achieve its tasks while being simultaneously observed and, if necessary, influenced by a higher level component to avoid unwanted emergent system states. To this end, an \emph{observer/controller architecture} has been proposed in \cite{RMB+06}. % that monitors and analyses the status of a \emph{system under observation and control} and influences its behaviour. 
Since it is in general impossible to foresee all possible configurations the \emph{system under observation and control} takes on in a dynamic environment, it is important to endow the system with \emph{learning components} so that system components can autonomously learn new control actions for unforeseen situations and evaluate their consequences. An XCS \cite{Wil95} is a rule-based evolutionary on-line learning system that is suitable to perform this learning task. It evolves a set of condition-action-mappings (called \emph{classifiers}) that contain a reward prediction estimating the benefits of their action for situations matching their condition. 
%new
In general, LCSs combine aspects of evolutionary algorithms, which are used for rule generation, with reinforcement learning, which is used for reward prediction (e.\,g., if an agent reaches a goal, a reward will be distributed among the classifiers). 
%In general, LCSs combine aspects of evolutionary algorithms that are used for rule generation with reinforcement learning that is used for reward prediction. I.\,e., if an agent reaches a goal, a reward will be distributed among the classifiers. 
Usually, this depends on the type of scenario: In \emph{single-step} environments, a reward is generated at each learning iteration, while in \emph{multi-step} environments (which are often characterized by local and restricted knowledge) 
%new
multiple steps are usually required to solve a problem and to generate a reward.
%the agent will construct the global information about the best classifiers % condition-action-mapping 
%using multiple % repetitions and 
%iterations of the whole learning problem.

Here, LCSs are used to learn control rules for agents in a \emph{predator/prey scenario}~\cite{BJD86}. Following the global task to observe one or up to $n$ (randomly) moving preys, a number of predators has % have
to achieve a common goal with local and distributed behavior (i.\,e., maximizing the global \emph{observation time}). The scenario shows technical relevance to organic computing scenarios concerning the learning aspect, it seems to be very flexible in its parametrization, and it allows many variations \cite{SV00}. % : Depending on the playground the agents have to live in, implementations can vary the number of agents, the number of targets, the number and distribution of obstacles, the moving strategies, the learning behaviour of the agents, the definition of local neighbourhoods, the communication possibilities between the agents, and various forms of collaboration and collective learning could be analysed. 
%E.\,g., agents can learn strategies for any given group behavior and they can exchange locally learned knowledge -- to benefit from their swarming behavior or to adapt quicker than in a single-agent case. 
Thus, the challenge of concurrent learning agents is specially addressed, where all predators are equipped with a single, independent XCS.
% In this paper a predator/prey scenario~\cite{BJD86} in the connection with a learning classifier system (LCS) is examined. Following the global task to observeone or up to $n$ (randomly) moving targets/preys, a number of rovers/predators has to achieve a common goal with local and distributed behaviour, e.\,g., maximizing the global observation time or catching moving target(s). The scenario shows technical relevance to Organic Computing scenarios and it seems to be very flexible in its parametrisation. Especially, it allows many variations: Depending on the playground the agents have to live in, implementations can vary the number of agents, the number of targets, the number and distribution of obstacles, the moving strategies, the learning behaviour of the agents, the definition of local neighbourhoods, the communication possibilities between the agents, and various forms of collaboration and collective learning could be analysed. 
% E.\,g., agents can learn strategies for any given group behavior and they can exchange locally learned knowledge -- to benefit from their swarming behavior or to adapt quicker than in a single-agent case. Here, the challenge of concurrent learning agents is addressed, where all agents are equipped with a single, independent LCS. 
%cf.\current field of research in the field of LCSs are the so-called \emph{eXtended Classifier Systems} (XCS)~\cite{Butz2006}\cite{BW02}\cite{Wil95}. Basically an XCS is a LCS, i.\,e., a number of rules, so-called \emph{classifiers}), each consisting of a condition/action pair. When the sensors of an agent match a condition then the action is executed. As wild-cards can be part of the condition a mechanism usually has to select from multiple classifiers. The classifiers are updated and adapted to the environment step by step using \emph{reinforcement learning}. When an agent reaches a goal condition a reward is distributed among the classifiers. The way this is done usually depends on the type of scenario, in a single-step environment a reward is generated at each step while in a multi-step environment with local information the agent has to construct the global information using multiple repetitions and steps.
Since neither the classical single-step nor the multi-step implementation of an XCS \cite{BW02} can be used to learn a \emph{dynamic observation task} % in a predator/prey scenario with obstacles 
(see argumentation in Section~\ref{subsection:scenario-classification}), the special focus of this paper is on an approach, how to handle such dynamic predator/prey scenarios. Thereby, the proposed idea is mainly based on a \emph{local cooperative reward function} and some kind of a \emph{temporary memory}. This memory stores a number of past action sets and their corresponding reward values. 
%new
Although LCSs have been investigated in dynamic environments \cite{Lan98,LW00}, specifically predator/prey scenarios have not really been addressed in XCS literature. % While there are works about LCSs in dynamic environments \cite{Lan98,LW00} specifically predator/prey scenarios have not really been addressed in XCS literature. % The discussion of such scenarios is an addition to existing literature about XCSs in dynamic scenarios  because predator/prey scenarios have not really been addressed in XCS literature.

% using \emph{not} global communication and organisation, e.\,g., as presented in \cite{TTS01}. % While there are implementations to handle such scenarios, they are either very simple with a goal object that can only be moved by agents, with only one or two agents and no obstacles~\cite{1102281} or use global communication and organization with a shared global classifier set~\cite{TTS01}.

% Thus, a promising modified XCS approach has been investigated to overcome the drawbacks of the classical XCS algorithm. The proposed idea is mainly based on a local cooperative reward function and some kind of temporary memory, which stores past actions sets and their corresponding reward values. Thus, local payoffs can be delayed and the reward function reflects in a better way the local agent behaviour. Cooperation (incorporated in the reward function) is more or less achieved through rejection and attraction. Predators reject each other, the prey attracts the predators. Thus, agents try to uniformly distribute on the grid and observation time of the prey seems to be maximized. In addition it is shown that the new XCS approach retains its ability to recognize local obstacle configurations in order to reach the goal object.

Thus, the remainder of this paper is structured as follows: First, LCSs are shortly introduced and their usability in different scenarios is discussed in Section~\ref{section:learning-classifier-systems}. 
%In Section~\ref{section:the-predator-prey-example}, the predator/prey example is described, which has not really been addressed in XCS literature as yet. %(in connection with XCS) new type of scenario (see Section~\ref{section:the-predator-prey-example}) is then presented and classified. 
Additionally, the characteristics of a predator/prey scenario are presented, which require a new reward function, as introduced in Section~\ref{section:the-reward-function}. Using this new reward function, several experiments are executed according to the methodology, as described in Section~\ref{section:methodology}. Experimental results are presented in Section~\ref{section:experimental-results}. The paper concludes with a summary and a discussion of future work in Section~\ref{section:conclusion}. % The paper concludes with Section~\ref{section:conclusion} with the result that the new approach shows significant improvements over the standard implementation but still need further research.
