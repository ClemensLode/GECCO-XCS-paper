\section{Learning Classifier Systems}
\label{section:learning-classifier-systems}

The field of LCSs, introduced in the 1970ies \cite{Hol75,Hol76,HR78}, is one of the most active and best-developed form of genetic based machine learning \cite{Kov02a,KL00,Lan08}. As mentioned above, much of LCS's theory is inherited from the reinforcement learning literature. % The following section provides a brief overview what an LCS is. 

LCSs are rule-based on-line learning systems that combine nature-inspired optimization heuristics and reinforcement learning techniques to learn appropriate actions for any input they get \cite{Wil95}. They are applicable to any problem, where a numerical reward reflecting the quality of an action can be obtained. The core component of an LCS is its \emph{rule base} that contains rules (called \emph{classifiers}) consisting of a condition, an action, a prediction value, etc. The selection of an appropriate action for a given input is a two-step process. From the rule base of all classifiers a subset called \emph{match set} is computed that contains all classifiers, whose condition matches the current input. Then, for all distinct actions present in the match set the average prediction of all classifiers advocating the same action is calculated. The action with the highest average prediction is selected for execution and all classifiers in the match set advocating that action form the \emph{action set}. The reward received from the environment is subsequently used to update the prediction values of all classifiers in the action set.

Classifiers forming the rule base are created in two different ways: Whenever the match set is empty, classifiers consisting of a condition matching the current input, a random action, and a default prediction value are inserted into the rule base by a process called \emph{covering}. Furthermore, a randomized \emph{evolutionary component} occasionally selects relatively good classifiers to be the parent individuals for a reproduction cycle. Crossover and mutation are applied to copies of the parents to form offspring, which are inserted into the rule base. 

A variety of different LCS implementations has been proposed (cf.~\cite{Kov02a}), many are based on Wilson's XCS \cite{Wil95} system, which is an LCS implementation that maintains separate prediction and fitness values and where the fitness of a classifier is based on the \emph{accuracy} of the prediction reward.  
% While Wilson originally used a binary input encoding for XCS, different approaches to represent real-valued inputs have been examined (e.\,g., \cite{DAL05a}). % (\eg, \cite{DAL05a,SB03,Wil00a}).

% The architecture of an LCS is simple to study and the knowledge is encoded and stored in so-called \emph{classifiers}. These classifiers consist of a condition part, which is matched against the input from the environment, an action part, and some kind of fitness or strength value.

% The strength value is used to decide, which classifier should be chosen if more than one classifier matches the current input. The encoding of conditions is done in such a way that different levels of \emph{generality} are possible, hence the range of input values each classifier matches against may vary from a single point to the entire search space. In a step that is called \emph{rule discovery} new classifiers are generated using genetic operators, like crossover and mutation, on existing classifiers, changing both condition and action. Furthermore, every time no classifier matching the current input is available, one or more classifiers with a matching condition and randomly chosen action are created (this is called \emph{covering}).

% After a classifier has been generated, the system has to determine its strength value. Every time a classifier's action is chosen, the strength value of this classifier is updated using some objective function. 

In certain problems (see Section~\ref{subsection:single-step-vs-multi-step-problems}), the environment's reaction on an action executed by an LCS is not instantaneous. Thus, further reinforcement learning cycles are required to build up an optimal (local) reward function (see Section~\ref{subsection:learning-markov}). Also, more complex scenarios with aliasing positions require additional handling like an internal memory (see Section~\ref{subsection:non-markov-environments}). In dynamic scenarios, e.\,g., the predator/prey scenario, additional measures are required. This will be subject of the following sections.

% The simplest way to increase performance would be to try and find classifiers, which maximize the objective function's value. Another approach more suitable for complex problems is the use of three distinct components as \emph{strength value}: The prediction $p$ of the objective function's value after this classifier's action has been executed, the average error of this prediction $\epsilon$, and a fitness value $F$ computed from this prediction error. The prediction is used to choose an action for execution: The classifier with best prediction, weighted by its fitness, is likely to result in best performance. Furthermore, the fitness is used to find classifiers suitable as input for the generation of new classifiers using genetic operators. The goal is to represent the entire search space with as few classifiers as possible while keeping only those that are more \emph{accurate} in their prediction. This approach is realized in \emph{XCS, the eXtended Classifier System} \cite{Wil95}, which is used within this paper and described in \cite{But00,BW02}.

\subsection{Single-Step vs.\ Multi-Step Problems}
\label{subsection:single-step-vs-multi-step-problems}

Literature about LCSs could be divided into \emph{single-step} and \emph{mul\-ti-step} approaches. This separation bases on how to solve the reinforcement learning problem and addresses a design decision, which has to be taken when implementing an LCS. It refers to the question, \emph{when} a reinforcement signal (reward) is achieved from the environment and \emph{how} this reward is distributed on the past action(s). 

In \emph{single-step} environments, the external reward is received on every time step and the environmental input for each time step has completely been independent of the prior time step in the case of environments with only a single entity with an LCS. When a decision is made, the reinforcement is directly received and measures the quality of the decision. Single-step environments generally involve categorization of data examples. A typical single-step benchmark problem is the \emph{boolean multiplexer problem} \cite{BKLW04,Wil95}. 

In \emph{multi-step} environments, the external reward may not necessarily be received on any time step, since the environmental input on a time step depends on at least the prior input and the system's last action. Typical multi-step environments are known as sequential environments or so-called \emph{Maze} problems, e.\,g., \emph{Wood1}~\cite{Wil94} or \emph{Wood2}~\cite{Wil95}. These examples model the adaptive interaction of an agent with its environment and have been studied using a variety of methods. Most often, a Maze is defined as a given number of neighboring cells in a grid-world. A cell is a bounded space formally defined and is the elementary unit of a Maze. Cells are either empty or can contain an obstacle, food, a so-called \emph{animat}, and eventually a predator of the animat. 

An animat is randomly placed in the Maze environment (which could be some kind of a labyrinth) and it tries to set its position to a cell containing food, which is sparsely located in the environment. To perform this task, it possesses a limited perception of the environment (often limited to the eight cells surrounding the animat's position) and it can also only move to an empty neighboring cell. Moving step by step through the Maze in order to fulfil its goal, the animat searches for a strategy (or adopt a policy), which minimizes the effort undertaken to find the food in the selected Maze environment. Maze environments offer plenty of parameters that allow to evaluate the complexity of a given system and also to evaluate the efficiency of a learning method. A full description of these parameters is available in \cite{BZ05}.

\subsection{Learning in Markov Environments} % with Markov Property}
\label{subsection:learning-markov}

When the environment in question does have the Markov property, i.\,e., it is an environment, where the sensory data for an agent differs for each position, then an agent can acquire global information by visiting all positions of the environment. With this information, an LCS can find the optimal set of rules to find the goal on the shortest route from any starting position. The learning process itself is done by a random walk (the \emph{explore} phase) in order to cover the search space, starting from a random position and repeating the process when reaching the goal position. Actions that lead to the goal are positively rewarded. Each action is rewarded by a portion of the reward value of the next action (or the maximal reward value when reaching the goal). The actual quality (the shortness of the path length) of the LCS is determined in the next phase (the \emph{exploit} phase), where the agent only chooses actions with the highest product out of fitness and prediction values.

\subsection{Non Markov Environments}
\label{subsection:non-markov-environments}

Some Maze problems offer perceptually identical situations that require different actions to reach the food. This problem is often studied in the context of \emph{non Markov environments}. \emph{Woods101} (see Figure~\ref{figure:woods101a}) is a typical example of a \emph{partially observable Markov decision process (POMDP)}, where a \emph{single-agent} cannot distinguish different situations due to a lack of global environmental information. 

The animat is placed in a free random field, it can sense the eight adjacent cells, and has to learn to reach the food denoted with $F$, while trees $T$ blocking any movement. Here, the aliasing positions marked with 1 and 2 share the same sensory configuration, but require different optimal actions (cf.\ Figure~\ref{figure:woods101b}, each optimal action for the different positions is marked with an arrow). In position 1, the optimal action is to move to the bottom right, while the optimal action in position 2 is to move to the bottom left. Thus, an XCS cannot evolve an optimal policy for \emph{Woods101} (without further modifications).

I.\,e., using records of past condition-action-mappings by adding temporary memory is a widespread approach to cope with such environments, as investigated in \cite{Lan98,LW00}.

\begin{figure}[ht]
  \subfigure[The aliasing positions are marked with 1 and 2.]{
  	\label{figure:woods101a}
  	\includegraphics[width=0.22\textwidth]{woods101a.eps}}\hfill
  \subfigure[Sensory configuration of the two aliasing positions]{
  	\label{figure:woods101b}
  	\includegraphics[width=0.22\textwidth]{woods101b.eps}}\hfill
  \caption{\mathversion{bold}The \emph{Woods101} environment: Trees are marked with $T$, food is marked with $F$.}
  \label{figure:woods101}
\end{figure}

A second non Markov property is still embedded in \emph{multi-agent} environments and this is related to a change of an agent's internal state. In scenarios with more than one learning agent, an agent has to evaluate actions that may be caused by its own internal state or that are the result of other agent's actions. It is difficult to recognize an environmental change, which is caused by the change of another agent's internal state, due to a lack of the other agents' informations. Even if an agent stays in the same location, the agent cannot evaluate the environmental changes. In \cite{TTS01}, this second non Markov property is defined as the \emph{non observable Markov decision process (NOMDP)}. 

In Section~\ref{subsection:scenario-classification}, it will be shown that the predator/prey scenario, presented in the following and used as testbed in this paper, includes both non Markov properties (POMDP and NOMDP). Thus, learning in such environments is more complex than learning in single-agent environments (where only one agent adapts to its dynamically changing environment) and requires a different approach, which is discussed in Section~\ref{section:the-reward-function}.
